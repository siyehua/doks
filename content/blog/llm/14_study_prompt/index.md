---
title: "ChatGPT 提示工程学习笔记"
description: "GPU,AI,AI绘画,AIGC,Developer"
summary: "AIGC 学习指路"
date: 2024-05-21T11:14:31+08:00
lastmod: 2024-05-21T11:14:31+08:00
draft: false
weight: 21
categories: []
tags: []
contributors: []
pinned: false
homepage: false
---


[Prompt: 面向开发者的 Prompt 工程 - AI 时代，必须掌握的 Prompt](https://github.com/GitHubDaily/ChatGPT-Prompt-Engineering-for-Developers-in-Chinese)


## 使用分割符来分割提示词注入
![img.png](img.png)

例子中，通过 \`\`\` 来分割用户的输入，避免自己的默认 Prompt 与用户输入的文本混淆，引起 GPT 的混乱，有了这个分割符之后，用户想注入或拦截原来的提示词，就变得困难：

![img_1.png](img_1.png)


这里例子中，当用户尝试让 GPT 忘记之前的 Prompt 的时候，由于我们已经限定了输入，GPT 就不会执行用户的 Prompt，但是要注意的是，我们使用的是 \`\`\`
 来进行分割的，当用户使用 \`\`\` 的时候，Prompt 仍然有可能会被破坏，为了避免这种情况，可以使用特殊的符号作为分割符，或者将用户输入的内容进行转义，
禁止用户直接输入 \`\`\`, 直接将其转义，这与 Sql 的注入攻击是类似的


## 原则 1：给出清晰具体的例子
![img_2.png](img_2.png)

### 策略1：使用上文说得各种分割符了进行分割
### 策略2：要求输入标准格式
一般来说要求输出 Json 格式
### 策略3：检查是否满足条件
GPT 每次输入的并不一定的是标准的 Json 格式，可能是数据不满足，也可能是 GPT 抽风了，
不管是什么原因，我们要对结果保持不信用，应该去检查结果。

如果结果不满足，我们可以要求它返回一个错误。

比如你给的文本或数据中，如果不满足结果，可以要求它输入：*Error Result* 来表示得不到正确的结果

### 策略4：告诉 GPT 一个具体的例子

要告诉 GPT一个成功的样例，然后让 GPT 按照这个样例进行输出

## 给 GPT 思考的时间
如果是比较复杂的例子，需要一步步的告诉 GPT 应该如何完成任务。

比如现在有一个任务，有两个表格，表格 1 有学生的学号，姓名，表格 2 有学生的学号，成绩，现在我们需要将表2 中的成绩
填写到表 1 中。

这个需求，通过写 excel 公式当然是可以做到的，但是大部分人都不会，我们可以这么教 GPT。

先上传这两个表格数据，然后编写 Prompt：
```
有两个表格，表格 1 有学生的学号，姓名，表格 2 有学生的学号，成绩，现在需要将表2 中的成绩
填写到表 1 中。

1. 先得到表格 2中第一行 id 等于 348943， 成绩 96
2. 再在表格 1 这中找到 id 等于 348943 所在的行，假设这个行是 13 行
3. 再在13行的名字后一列，输入第1 步得到的 成绩 96

请按照上面的步骤，帮我完成表格 1 中所有的成绩填写,生成新的表格
```


![img_3.png](img_3.png)

## 避免幻觉问题

比如你问 GPT 日本是中国的哪个省，这当然是不可能的。GPT 会瞎回答（升级后的模型可能会告诉你这是常识错误）。
这个例子比较简单，但是当我们遇到非常复杂的问题的时候：

比如在编程领域，我们问 GPT 如何使用某个类去设置什么属性，其实这个类根本就没有提供，这时候，它就可能胡编乱造。

有以下几个方式来尽量避免幻觉问题：
1. 尽量使用更先进的模型
2. 可以告诉它更多的上下文
3. 告诉它，如果遇到错误或无法回答，输入特定的错误结果，而不是瞎编


## 分析迭代过程

总结就是使用 LLM 的时候，要不停地优化自己的Prompt ,通过结果与错误来改善自己的 Prompt：

![img_4.png](img_4.png)

比如控制输入的内容，在最后输入 id，型号。
或者让 GPT 输入 300 个单词，3 句话等等

## Temperature 温度

可以理解成随机种子的随机性，Temperature 是 0 ~ 1 之间，如果你希望回答比较固定，那就可以调整 Temperature 趋向于 0，
如果你希望回答比较具有随机性，想象力，则让 Temperature 趋向于 1.

上文中，返回 json 的例子，并不总是都是稳定的，有可能 AI 并没有返回 json，这个时候我们可以降低 Temperature 来让 AI 返回的
json 概率更高。

## 对话的时候可以添加不同的身份

![img_5.png](img_5.png)
